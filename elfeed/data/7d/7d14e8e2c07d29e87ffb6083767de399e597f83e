<p>While working on <a href="https://sr.ht/~ntietz/isabella-db/">a project</a> where I was putting a lot of data into a HashMap, I started to notice my hashmaps were taking up a lot of RAM.
I mean, a <em>lot</em> of RAM.
I did a back of the napkin calculation for what the minimum memory usage should be, and I was getting <strong>more than twice what I expected</strong> in resident memory.</p>
<p>I'm aware that HashMaps trade off space for time.
By using more space, we're able to make inserts and retrievals much more efficient.
But how <em>much</em> space do they trade off for that time?</p>
<p>I didn't have an answer to that question, so I decided to measure and find out.
If you <strong>just want to know the answer, skip to the last section</strong>; you'll know you're there when you see charts.
Also, all the <a href="https://git.sr.ht/~ntietz/rust-hashmap-overhead">supporting code</a> and <a href="https://docs.google.com/spreadsheets/d/1jWv3nzQwncXy0xK_MKmcUkflT8sbQa02skNxP609az4/edit?usp=sharing">data</a> is available if you want to do your own analysis.</p>
<h1 id="allocators-in-rust">Allocators in Rust</h1>
<p>Rust takes care of a lot of memory management for you.
In most cases, you don't need to think about the allocation behavior:
Things are created when you ask for them, and they're dropped when you stop using them.
The times when you <em>do</em> have to think about it, the borrow checker will usually make that clear to you.</p>
<p>Sometimes, though, you get into situations where memory allocation behavior matters a lot more for your system.
This can be the case if you're very memory constrained (as I was) or if you are trying to avoid the cost of memory allocation.
In these situations, Rust lets you define your own allocator with the behavior you want!</p>
<p>The <a href="https://doc.rust-lang.org/std/alloc/struct.System.html">System</a> allocator is the default allocator used by Rust programs if you don't do anything special.
It uses the default allocator provided by your operating system, so it's using <code>malloc</code> under the hood on Linux systems.</p>
<p>Another one I've seen referenced a lot is <a href="https://crates.io/crates/tikv-jemallocator">tikv-jemallocator</a>, which provides a different <code>malloc</code> implementation with some characteristics like avoiding fragmentation.
It comes from FreeBSD.
I didn't explore using this one other than idly trying it in my main project, where it didn't make any discernible difference in memory overhead<sup class="footnote-reference"><a href="#1">1</a></sup>.</p>
<p>There are some other fun allocators, too, and you can do some really neat things with them.
Here are two that I thought were neat:</p>
<ul>
<li><a href="https://crates.io/crates/bumpalo">bumpalo</a> has a cute name and is a bump allocator that can allocate super quickly, but generally cannot deallocate individual objects; niche in use</li>
<li><a href="https://crates.io/crates/wee_alloc">wee-alloc</a> is also cutely (and descriptively!) named and is a &quot;simple, correct implementation&quot; of an allocator for WASM targets, so it generates small code for allocations</li>
</ul>
<p>There are also a few allocators which help you measure overhead.
But where's the fun in that???
Let's do it ourselves!</p>
<h1 id="writing-an-allocator-to-track-allocations">Writing an allocator to track allocations</h1>
<p>It's tricky writing an allocator that does the useful work of allocation, and there's a lot of nuance.
It's a lot easier to write an allocator that wraps around an existing one and records measurements!
That's what we're doing.</p>
<p>The thing to know is that we need to implement the <a href="https://doc.rust-lang.org/std/alloc/trait.GlobalAlloc.html"><code>GlobalAlloc</code></a> trait.
It has two methods we have to define: <code>alloc</code> and <code>dealloc</code>.
We will make something very simple which just wraps <code>System</code> without doing anything at all besides passing through data to some record functions.</p>
<p>We start with a struct.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">&#x2F;&#x2F;&#x2F; TrackingAllocator records the sum of how many bytes are allocated
&#x2F;&#x2F;&#x2F; and deallocated for later analysis.
struct TrackingAllocator;
</code></pre>
<p>Note that our struct doesn't have any fields.
We can't put anything dynamic in it.
We'll need some atomic ints and such to track allocations.
Since we don't expect to have multiple of these at once, we'll just put those as statics in the module scope.
We could put the fields we want in the struct, but it makes constructing it a little more annoying and we won't have multiple allocators at once, so we're just going to make those statics.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">static ALLOC: AtomicUsize = AtomicUsize::new(0);
static DEALLOC: AtomicUsize = AtomicUsize::new(0);
</code></pre>
<p>And now we define <code>alloc</code> and <code>dealloc</code> so that <code>TrackingAllocator</code> is <code>GlobalAlloc</code>.
Implementing <code>GlobalAlloc</code> requires marking things <code>unsafe</code>.
What we're doing here isn't really unsafe, but we satisfy the interface.
All we're doing is passing through to <code>System</code> and recording it with some helper functions we'll define later.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">unsafe impl GlobalAlloc for TrackingAllocator {
    unsafe fn alloc(&amp;self, layout: Layout) -&gt; *mut u8 {
        let p = System.alloc(layout);
        record_alloc(layout);
        p
    }

    unsafe fn dealloc(&amp;self, ptr: *mut u8, layout: Layout) {
        record_dealloc(layout);
        System.dealloc(ptr, layout);
    }
}
</code></pre>
<p>Now we also have to define the helper methods to record allocations.
They're as straightforward as can be, just doing a <code>fetch_add</code> to record the size of the allocated or deallocated memory into its corresponding counter.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">pub fn record_alloc(layout: Layout) {
    ALLOC.fetch_add(layout.size(), SeqCst);
}

pub fn record_dealloc(layout: Layout) {
    DEALLOC.fetch_add(layout.size(), SeqCst);
}
</code></pre>
<p>Now the functionality for the allocator itself is basically in place, and we can move on to using it!</p>
<h1 id="using-our-allocator">Using our allocator</h1>
<p>There are two things we need to do to use our allocator: Set it up as the global allocator, and add a little bit of functionality to get <em>useful</em> data out.</p>
<p>Let's make it the global allocator first.
This is the easy bit.
Somewhere in your program (such as in <code>main.rs</code>), you create an instance and mark it as the global allocator:</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">#[global_allocator]
static ALLOC: TrackingAllocator = TrackingAllocator;
</code></pre>
<p>And now that's done.
That's all you need to do to change the global allocator!
You can see also why we made initialization as easy as possible.</p>
<p>Now to address the ergonomics of use.
As it stands, <em>every</em> allocation and deallocation will get recorded.
That's not quite what we want.
We want to isolate certain pieces of the program to measure their allocation separately from test setup and teardown.
We also want to record stats from multiple separate runs and report them nicely.</p>
<p>The first thing to do is define a struct for the stats we want to return.
We want the total allocation and deallocation, and it would also be convenient to have their difference.
This can be calculated later, but let's just include it in the struct for now.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">pub struct Stats {
    pub alloc: usize,
    pub dealloc: usize,
    pub diff: isize,
}
</code></pre>
<p>And now we need some helper methods to reset the counters, and get our stats out.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">
pub fn reset() {
    ALLOC.store(0, SeqCst);
    DEALLOC.store(0, SeqCst);
}


pub fn stats() -&gt; Stats {
    let alloc: usize = ALLOC.load(SeqCst);
    let dealloc: usize = DEALLOC.load(SeqCst);
    let diff = (alloc as isize) - (dealloc as isize);

    Stats {
        alloc,
        dealloc,
        diff,
    }
}
</code></pre>
<p>And we have the pieces we need to use this nicely!
We can call <code>reset()</code> to clear the values before an experiment, and call <code>stats()</code> to get them afterwards.</p>
<h1 id="putting-together-the-pieces">Putting together the pieces</h1>
<p>Let's put together the pieces now and measure the overhead of <code>HashMap</code>s!
As a bonus, we'll also measure the overhead of <code>BTreeMap</code>s.</p>
<p>First let's define a helper function that lets us measure and report on the allocations from a test scenario.
This function should take in another function, which will return some data (this is important so that the data <em>isn't dropped</em> until after the measurement is complete, or the diff will be inaccurately low).
The job of this function is to reset the allocator, run the function, report the stats, then drop the data.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">pub fn run_and_track&lt;T&gt;(name: &amp;str, size: usize, f: impl FnOnce() -&gt; T) {
    alloc::reset();

    let t = f();

    let Stats {
        alloc,
        dealloc,
        diff,
    } = alloc::stats();
    println!(&quot;{name},{size},{alloc},{dealloc},{diff}&quot;);

    drop(t);
}
</code></pre>
<p>For simplicity we're just printing the results to <code>stdout</code>, and the CSV header will be defined elsewhere.</p>
<p>Now let's define our scenarios.
For this, we'll first assume that we have constructed some data:</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">let pairs = generate_keys_values(1_000_000);
</code></pre>
<p>There's a helper function that fills a <code>Vec</code> with as many key/value pairs as we want.
Each is a pair of a random <code>u64</code> (key) and a 100-byte random <code>u8</code> blob (value).
The particular data here shouldn't matter too much, but I picked something of about 100 bytes to match the domain I originally saw this in.</p>
<p>We'll also have a list of sizes for the tests; later, we can just assume we have a <code>usize</code> called <code>size</code> which we can use.
You can see the full details in the <a href="https://git.sr.ht/~ntietz/rust-hashmap-overhead/tree/main/item/src/main.rs">complete listing</a>.</p>
<p>Now let's define the baseline.
The baseline here is two <code>Vec</code>s, one of the keys and one of the values, constructed with <em>exactly</em> the capacity we need and no more.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">run_and_track(&quot;vec-pair&quot;, size, || {
    let mut k: Vec&lt;u64&gt; = Vec::with_capacity(size);
    let mut v: Vec&lt;DummyData&gt; = Vec::with_capacity(size);

    for (key, val) in &amp;pairs[..size] {
        k.push(*key);
        v.push(*val);
    }

    (k, v)
});
</code></pre>
<p>And now we can also define our BTree and HashMap scenarios.</p>
<pre data-lang="rust" class="language-rust "><code class="language-rust" data-lang="rust">run_and_track(&quot;hashmap&quot;, size, || {
    let mut m = HashMap::&lt;u64, DummyData&gt;::new();

    for (key, val) in &amp;pairs[..size] {
        m.insert(*key, *val);
    }

    m
});

run_and_track(&quot;btreemap&quot;, size, || {
    let mut m = BTreeMap::&lt;u64, DummyData&gt;::new();

    for (key, val) in &amp;pairs[..size] {
        m.insert(*key, *val);
    }

    m
});
</code></pre>
<p>When we run these (with some additional glue code), we'll get a CSV as output which we can then load into a spreadsheet and analyze.</p>
<h1 id="the-results-i-brought-charts">The results (I brought charts)</h1>
<p>The results surprised me, because I (naively, perhaps) expected the HashMap to maintain fairly constant, fairly low overhead.
I was aware that hashmaps in general have a &quot;load factor&quot;, but I didn't fully understand how it was utilized.
It is used to define when the HashMap will resize to contain more elements.
If your load factor is 1, then it will reallocate when the map is full.
I think the load factor for Rust's HashMap is something like 7/8. This means that when it has 12.5% capacity remaining, it will reallocate (and probably double, so that the amortized cost of reallocating is O(1)).</p>
<p>If we do some analysis, we can reach a better estimate than my naive unthinking estimate that it would have 12.5% overhead.
In fact, it's much higher than that.
If the HashMap doubles its capacity when it hits 12.5% remaining (14% overhead), then after doubling it will have 56% free capacity, and the overhead of the extra space is about 125% of the used space.
On average, we expect the overhead to be somewhere between those, perhaps around 70%.</p>
<p>How does this compare to what we see in this test?</p>
<p>First we can see the growth behavior of both containers against the baseline:</p>
<p><img src="/images/hashmap-btree-growth.svg" alt="Chart of growth in memory usage of HashMaps and BTreeMaps against a baseline" /></p>
<p>Here we can see that BTreeMaps grow smoothly linearly with the size of the data, while HashMaps are growing stepwise.
Additionally, it looks like HashMaps are almost always using more memory than BTreeMaps.</p>
<p>We can see the trends more clearly if instead of the direct memory usage, we plot the <em>overhead</em>: as a ratio, how much additional memory is it using compared to the baseline?
For the baseline, the answer is 0.
From our analysis, we expect the hashmap to average about 0.7.</p>
<p><img src="/images/hashmap-btree-overhead.svg" alt="Chart of the overhead ratio of HashMaps and BTreeMaps against a baseline" /></p>
<p>And here we see the behavior more clearly.
BTreeMaps do indeed have fairly consistent overhead.
On the other hand, HashMaps' overhead swings wildly.
It goes up over 1.25 (about what we hypothesized), and drops as low as about 0.125 (also what we hypothesized).</p>
<p>And if we average it? <strong>0.73</strong>.
The hypothesis was bang on.</p>
<p>So in general, you can expect to allocate <strong>nearly twice as much memory as your elements alone</strong> if you put them in a Rust HashMap, and about <strong>50% extra memory</strong> if you put them in a BTreeMap.</p>
<p>Hashmaps make a clear space-for-time tradeoff, and it's easier to make that tradeoff effectively if you know how <em>much</em> of each you're trading off!
Measuring the time tradeoff is left as an exercise for the reader ðŸ˜‰.</p>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>I tried this when someone suggested my high resident memory might be from fragmentation, since jemalloc is better at avoiding fragments.
This was before I realized the extent of the overhead of HashMaps, but it did lead me down this allocator journey.</p>
</div>
